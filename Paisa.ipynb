{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac60cd5-9285-4692-8a18-5453041c441e",
   "metadata": {},
   "outputs": [],
   "source": [
    " def _compute_z(theta_0, theta_weights, x_sample):\n",
    "    \"\"\"\n",
    "    Computes the linear part: z = theta_0 + (theta_weights . x_sample)\n",
    "    p.s. We have used theta_1 and X as the parameter names during the class.\n",
    "    the purpose of which is to make you understand that whichever names a variable might be of,\n",
    "    the concepts remain the same.\n",
    "    \"\"\"\n",
    "    # First, check for compatible lengths |\n",
    "    # i.e., \"Cardinality\" as discussed during the class.\n",
    "    if len(theta_weights) != len(x_sample):\n",
    "        raise ValueError(\"\"\"Mismatch in length of weights and features\\n\n",
    "                            Or, Mismatch in the length of theta_1 and X\"\"\")\n",
    "    \n",
    "    z = theta_0\n",
    "    for i in range(len(theta_weights)):\n",
    "        z += theta_weights[i] * x_sample[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c4245-8d9c-4fce-be30-b7905d6cba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    " \n",
    "def _sigmoid(z):\n",
    " \"\"\"\n",
    " The Sigmoid activation function.\n",
    " Includes guards for numerical stability to prevent overflow.\n",
    " \"\"\"\n",
    " if z > 700:  # e^(-700) is effectively 0\n",
    " return 1.0\n",
    " elif z < -700: # e^(700) is effectively infinity\n",
    " return 0.0\n",
    " else:\n",
    " return 1.0 / (1.0 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca739e6-e019-432b-8655-981c9de9217e",
   "metadata": {},
   "outputs": [],
   "source": [
    " def _predict_probability(theta_0, theta_weights, x_sample):\n",
    " \"\"\"\n",
    " Our full hypothesis: h(x) = sigmoid(z)\n",
    " \"\"\"\n",
    " z = _compute_z(theta_0, theta_weights, x_sample)\n",
    " return _sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34f46265-27ea-4a64-b554-386371d7a34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing CoreLogisticRegression ---\n",
      "Starting training...\n",
      "Iteration 0/5000: Cost = 0.6931\n",
      "Iteration 500/5000: Cost = 0.1241\n",
      "Iteration 1000/5000: Cost = 0.0713\n",
      "Iteration 1500/5000: Cost = 0.0510\n",
      "Iteration 2000/5000: Cost = 0.0401\n",
      "Iteration 2500/5000: Cost = 0.0332\n",
      "Iteration 3000/5000: Cost = 0.0284\n",
      "Iteration 3500/5000: Cost = 0.0249\n",
      "Iteration 4000/5000: Cost = 0.0222\n",
      "Iteration 4500/5000: Cost = 0.0200\n",
      "Iteration 5000: Final Cost = 0.0183\n",
      "Training complete.\n",
      "\n",
      "Final Bias (theta_0): -9.8485\n",
      "Final Weights (theta_1): 2.9078\n",
      "\n",
      "--- Test Results ---\n",
      "Input: 0.5 hours | Prob(Pass): 0.0002 | Prediction: 0\n",
      "Input: 3.0 hours | Prob(Pass): 0.2450 | Prediction: 0\n",
      "Input: 3.5 hours | Prob(Pass): 0.5814 | Prediction: 1\n",
      "Input: 7.0 hours | Prob(Pass): 1.0000 | Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "\n",
    "# Set higher recursion limit for potential deep calls, though not strictly necessary for this class\n",
    "# sys.setrecursionlimit(2000)\n",
    "\n",
    "class CoreLogisticRegression:\n",
    "    \"\"\"\n",
    "    A basic implementation of Logistic Regression using Batch Gradient Descent.\n",
    "    It handles binary classification problems.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        \"\"\"\n",
    "        Initializes the model's hyperparameters and internal variables.\n",
    "\n",
    "        :param learning_rate: The step size for updating weights during optimization.\n",
    "        :param n_iterations: The number of times the model will update the weights.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.theta_0 = 0.0      # Bias term (intercept)\n",
    "        self.theta_weights = [] # Feature weights\n",
    "        self.cost_history = []  # To track the cost over iterations\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Computes the sigmoid function: h(z) = 1 / (1 + e^-z).\n",
    "        Includes checks for numerical stability to prevent math domain errors \n",
    "        when exp(-z) results in extremely large or small numbers.\n",
    "\n",
    "        :param z: The linear combination result (z = theta_0 + X * theta_weights).\n",
    "        :return: Probability value between 0.0 and 1.0.\n",
    "        \"\"\"\n",
    "        # For large positive z, math.exp(-z) approaches 0, so sigmoid approaches 1.0\n",
    "        if z > 700: \n",
    "            return 1.0\n",
    "        # For large negative z, math.exp(-z) approaches infinity, so sigmoid approaches 0.0\n",
    "        elif z < -700: \n",
    "            return 0.0\n",
    "        # Standard sigmoid calculation\n",
    "        else: \n",
    "            return 1.0 / (1.0 + math.exp(-z))\n",
    "    \n",
    "    def _compute_z(self, x_sample):\n",
    "        \"\"\"\n",
    "        Computes the linear combination of inputs and weights for a single sample (z).\n",
    "        This is the input to the sigmoid function.\n",
    "\n",
    "        :param x_sample: A single feature vector (list of features).\n",
    "        :return: The scalar value z.\n",
    "        \"\"\"\n",
    "        # Start with the bias term\n",
    "        z = self.theta_0\n",
    "        # Add the weighted sum of features\n",
    "        for i in range(len(self.theta_weights)):\n",
    "            z += self.theta_weights[i] * x_sample[i]\n",
    "        return z\n",
    "    \n",
    "    def _predict_probability(self, x_sample):\n",
    "        \"\"\"\n",
    "        Makes a probability prediction for a single sample.\n",
    "        \n",
    "        :param x_sample: A single feature vector.\n",
    "        :return: The predicted probability (h(x)).\n",
    "        \"\"\"\n",
    "        z = self._compute_z(x_sample)\n",
    "        return self._sigmoid(z)\n",
    "    \n",
    "    def _compute_cost(self, y_true, y_pred_probs):\n",
    "        \"\"\"\n",
    "        Computes the binary cross-entropy (Log Loss) for the entire dataset.\n",
    "\n",
    "        Cost = (-1/m) * sum(y * log(h) + (1-y) * log(1-h))\n",
    "\n",
    "        :param y_true: List of true target labels (0 or 1).\n",
    "        :param y_pred_probs: List of predicted probabilities (0.0 to 1.0).\n",
    "        :return: The average cost (loss) over all samples.\n",
    "        \"\"\"\n",
    "        m = len(y_true)\n",
    "        if m == 0: \n",
    "            return 0.0\n",
    "            \n",
    "        total_cost, epsilon = 0.0, 1e-9\n",
    "        \n",
    "        for i in range(m):\n",
    "            # Clipping the predicted probability (h) to prevent log(0) which is undefined.\n",
    "            h = max(epsilon, min(1.0 - epsilon, y_pred_probs[i]))\n",
    "            \n",
    "            # Binary Cross-Entropy formula for one sample\n",
    "            cost_sample = -y_true[i] * math.log(h) - (1 - y_true[i]) * math.log(1 - h)\n",
    "            total_cost += cost_sample\n",
    "            \n",
    "        # Return the average cost\n",
    "        return total_cost / m\n",
    "    \n",
    "    def _compute_gradients(self, X_data, y_true, y_pred_probs):\n",
    "        \"\"\"\n",
    "        Computes the gradients (partial derivatives) of the cost function \n",
    "        with respect to the bias (theta_0) and weights (theta_weights).\n",
    "\n",
    "        Gradient = (1/m) * sum((h(x) - y) * x_j)\n",
    "\n",
    "        :param X_data: The feature matrix.\n",
    "        :param y_true: List of true target labels.\n",
    "        :param y_pred_probs: List of predicted probabilities.\n",
    "        :return: A tuple containing the bias gradient and the list of weight gradients.\n",
    "        \"\"\"\n",
    "        m = len(y_true)\n",
    "        # Number of features\n",
    "        n_features = len(self.theta_weights)\n",
    "        \n",
    "        grad_theta_0 = 0.0\n",
    "        # Initialize gradients for all weights to zero\n",
    "        grad_theta_weights = [0.0] * n_features\n",
    "        \n",
    "        for i in range(m):\n",
    "            # The core error term for Logistic Regression: (Prediction - True Label)\n",
    "            error = y_pred_probs[i] - y_true[i]\n",
    "            \n",
    "            # Gradient for the bias (theta_0) - corresponding feature is always 1\n",
    "            grad_theta_0 += error\n",
    "            \n",
    "            # Gradients for the weights (theta_weights)\n",
    "            for j in range(n_features):\n",
    "                grad_theta_weights[j] += error * X_data[i][j]\n",
    "        \n",
    "        # Average the gradients over all training examples (m)\n",
    "        grad_theta_0 /= m\n",
    "        for j in range(n_features): \n",
    "            grad_theta_weights[j] /= m\n",
    "            \n",
    "        return grad_theta_0, grad_theta_weights\n",
    "    \n",
    "    def fit(self, X_data, y_data, verbose=True):\n",
    "        \"\"\"\n",
    "        Trains the model using batch gradient descent.\n",
    "\n",
    "        :param X_data: The feature matrix (list of lists).\n",
    "        :param y_data: The target vector (list of 0s and 1s).\n",
    "        :param verbose: If True, prints the cost every 10% of iterations.\n",
    "        \"\"\"\n",
    "        # Initialize weights based on the number of features in the input data\n",
    "        if not X_data:\n",
    "            print(\"Error: Input data X_data is empty.\")\n",
    "            return\n",
    "            \n",
    "        n_features = len(X_data[0])\n",
    "        self.theta_0 = 0.0\n",
    "        self.theta_weights = [0.0] * n_features\n",
    "        self.cost_history = []\n",
    "        \n",
    "        # Start Batch Gradient Descent training loop\n",
    "        for i in range(self.n_iterations):\n",
    "            # 1. Calculate the predicted probabilities for the current weights\n",
    "            y_pred_probs = [self._predict_probability(x) for x in X_data]\n",
    "            \n",
    "            # 2. Calculate the cost (Loss) and store it\n",
    "            cost = self._compute_cost(y_data, y_pred_probs)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # 3. Calculate the gradients (direction of steepest ascent)\n",
    "            grad_theta_0, grad_theta_weights = self._compute_gradients(X_data, y_data, y_pred_probs)\n",
    "            \n",
    "            # 4. Update the weights and bias using the gradient descent update rule\n",
    "            #   (New Parameter) = (Old Parameter) - (Learning Rate) * (Gradient)\n",
    "            \n",
    "            self.theta_0 -= self.learning_rate * grad_theta_0\n",
    "            for j in range(n_features):\n",
    "                self.theta_weights[j] -= self.learning_rate * grad_theta_weights[j]\n",
    "            \n",
    "            # Print cost periodically for monitoring\n",
    "            if verbose and self.n_iterations >= 10 and i % (self.n_iterations // 10) == 0:\n",
    "                print(f\"Iteration {i}/{self.n_iterations}: Cost = {cost:.4f}\")\n",
    "        \n",
    "        # Final cost display\n",
    "        if verbose:\n",
    "             print(f\"Iteration {self.n_iterations}: Final Cost = {self.cost_history[-1]:.4f}\")\n",
    "\n",
    "    def predict_proba(self, X_data):\n",
    "        \"\"\"\n",
    "        Predicts probabilities for new data points.\n",
    "\n",
    "        :param X_data: The feature matrix for prediction.\n",
    "        :return: A list of predicted probabilities (0.0 to 1.0).\n",
    "        \"\"\"\n",
    "        return [self._predict_probability(x) for x in X_data]\n",
    "        \n",
    "    def predict(self, X_data, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predicts class labels (0 or 1) based on a threshold.\n",
    "\n",
    "        :param X_data: The feature matrix for prediction.\n",
    "        :param threshold: The probability cut-off for assigning class 1.\n",
    "        :return: A list of predicted class labels (0 or 1).\n",
    "        \"\"\"\n",
    "        probabilities = self.predict_proba(X_data)\n",
    "        return [1 if prob >= threshold else 0 for prob in probabilities]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"--- Testing CoreLogisticRegression ---\")\n",
    "    \n",
    "    # 1. Create a simple 1D dataset (Hours Studied vs. Pass/Fail)\n",
    "    # The dataset suggests a clear separation point around 3.5-4.0 hours.\n",
    "    X_train = [[1.0], [1.5], [2.0], [2.5], [4.5], [5.0], [5.5], [6.0]]\n",
    "    y_train = [0, 0, 0, 0, 1, 1, 1, 1] # 0 = Fail, 1 = Pass\n",
    "    \n",
    "    # 2. Initialize and train the model\n",
    "    # Higher learning rate (0.1) and more iterations (5000) for fast convergence on small data\n",
    "    model = CoreLogisticRegression(learning_rate=0.1, n_iterations=5000)\n",
    "    print(\"Starting training...\")\n",
    "    model.fit(X_train, y_train, verbose=True)\n",
    "    print(\"Training complete.\")\n",
    "    \n",
    "    # 3. Print the final learned parameters\n",
    "    print(f\"\\nFinal Bias (theta_0): {model.theta_0:.4f}\")\n",
    "    # Since X_train is 1D, we check the first element of weights\n",
    "    if model.theta_weights:\n",
    "        print(f\"Final Weights (theta_1): {model.theta_weights[0]:.4f}\")\n",
    "    \n",
    "    # 4. Make predictions on new, unseen data\n",
    "    # 0.5 and 3.0 should be classified as 0 (Fail)\n",
    "    # 3.5 is the transition point\n",
    "    # 7.0 should be classified as 1 (Pass)\n",
    "    X_test = [[0.5], [3.0], [3.5], [7.0]]\n",
    "    probs = model.predict_proba(X_test)\n",
    "    labels = model.predict(X_test)\n",
    "    \n",
    "    print(\"\\n--- Test Results ---\")\n",
    "    for i in range(len(X_test)):\n",
    "        print(f\"Input: {X_test[i][0]} hours | \"\n",
    "              f\"Prob(Pass): {probs[i]:.4f} | \"\n",
    "              f\"Prediction: {labels[i]}\")\n",
    "\n",
    "    # Expected output:\n",
    "    # 0.5 -> Low Prob -> 0\n",
    "    # 3.0 -> Low/Mid Prob -> 0\n",
    "    # 3.5 -> Mid Prob (~0.5) -> 0 or 1 (depending on exact convergence)\n",
    "    # 7.0 -> High Prob -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5507a93f-cec4-4e8b-bce8-5e2c471b336c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
